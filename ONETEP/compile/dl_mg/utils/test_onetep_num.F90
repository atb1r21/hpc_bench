!> Program to test numerical behaviour of DL_MG (with defect correction) using
!! input/output data from ONETEP test cases.
!! This program is particularly concerned with testing the defect correction
!! and high-order finite differences parts of DL_MG with 3-D parallel data
!! decomposition, since ONETEP only allows a 1-D "slab" distribution of data
!! among MPI ranks.
!!
!! Reads OpenDX format files generated by ONETEP's visual_output_dx routine.
!! --> ONETEP version 4.5.2, with the following settings in input file:
!!     dx_format: T
!!     dx_format_digits: 16
!!     devel_code: MG:DEBUG_DUMP:MG
!!
!! This program cannot parse general OpenDX format files, and relies upon the
!! specific format of the data output by ONETEP.
!!
!! Adapted from the test_onetep_ls program.
!! James C. Womack, 2016
!!
!! TODO
!! * Support input of eps_half.dx files.
!! * Improve error-handling for MPI_* calls.
!! * Improve treatment of send/recv statuses in read_onetep_data.
!! * Allow user to specify number of significant digits in DX file on command line.
!! * Extract full grid size from DX file header in read_dx_data.
!! * Consider creating a Python wrapper script to allow automated repeated runs

program test_onetep_num
  use iso_fortran_env, only: ERROR_UNIT, OUTPUT_UNIT
  use mpi
  use dl_mg
  use dl_mg_defco, only: dl_mg_defco_init, dl_mg_defco_defect_corr_solver, &
    DL_MG_PBE_MODE_NONE, &
    DL_MG_OUTPUT_MODE_QUIET, DL_MG_OUTPUT_MODE_VERBOSE, &
    DL_MG_DEBUG_MODE_OFF, DL_MG_DEBUG_MODE_ON
  use dl_mg_params, only: wp
  implicit none

  integer :: local_comm !< MPI communicator derived from MPI_COMM_WORLD, but with
                     !! unneeded MPI ranks removed
  integer :: color   !< Used in MPI_COMM_SPLIT
  real(wp), parameter :: pi = 4.0d0 * atan(1.0d0)
  integer :: npt1, npt2, npt3 !< Full grid extents
  integer :: pq1f, pq2f, pq3f !< Truncated grid extents
  integer :: num_slabs12_pq, nproc, myid, mg_comm, ierr
  integer :: nproc_truncated
  integer :: nproc_active
  integer :: idx_s(3), idx_e(3), nfiles
  integer :: n_loc(3)
  integer, allocatable :: zsizes(:)
  real(wp) :: d1f, d2f, d3f
  real(wp), allocatable :: bound(:,:,:), pot(:,:,:), rho(:,:,:)
  !real(wp), allocatable :: res(:,:,:)
  real(wp), allocatable :: eps_half(:,:,:,:)
  real(wp), allocatable :: eps_full(:,:,:)
  character(len=10)  :: timetag
  real(wp) :: t1, t2, tsolver_min, tsolver_max, tsolver_sum
  real(wp) :: pot_norm_before, pot_norm_after, rho_norm
  ! Defect correction variables
  integer :: n(3), bc(3)
  real(kind=wp) :: d(3)
  integer :: order  ! Read from command line
  ! Defect correction parameters
  integer, parameter :: pbe_mode = DL_MG_PBE_MODE_NONE ! Poisson equation only
  logical, parameter :: is_damping = .false.   ! ONETEP default for pbe_mode = "NONE"
  real(kind=wp), parameter :: error_tol  = 1.0e-5_wp ! ONETEP default
  real(kind=wp), parameter :: defect_tol = 1.0e-2_wp ! ONETEP default
  integer, parameter :: maxit_defco = 100      ! ONETEP default
  integer, parameter :: agg_level = 0          ! ONETEP default
  ! Parallelism
  integer, allocatable :: global_map(:,:)
                   !< Global map for all MPI processes, containing start and
                   !! end indexes for each Cartesian direction
  integer, allocatable :: global_npoints(:,:)
                   !< Global array containing number of grids points in each
                   !! Cartesian direction for all MPI processes
  integer :: np(3) !< Number of MPI ranks in x, y, z directions in Cartesian
                   !! topology communicator, set on command line.
                   !! product( np ) should be equal to the number of active ranks
                   !! after MPI_COMM_SPLIT.
                   !! Additionally, the value of np in any direction should not
                   !! exceed the number of grid points - 2 in that direction.
  ! Command line arguments
  integer, parameter :: nargs = 5  !< Expected number of command line arguments
  integer, parameter :: cmd_str_len = 128
  integer, parameter :: fmt_str_len = 128
  character(len=cmd_str_len) :: model_name !< Character string to contain model name
  character(len=cmd_str_len) :: np_str(3)  !< Array of character strings to contain
                                           !! number of MPI ranks in x, y, z directions
  character(len=cmd_str_len) :: order_str  !< Array of character strings to contain
  character(len=fmt_str_len) :: fmt_np
  character(len=fmt_str_len) :: fmt_order
  ! Counters
  integer :: i, j, k
  integer :: ix, iy, iz
  integer :: irank
  integer :: icart, jcart, kcart
  integer :: np_count(3)
  integer :: total_points
  integer :: line_sum
  ! Buffers
  integer, parameter :: buff_len = 10
  character(len=buff_len) :: char_buffer

  ! Set format strings
  write(char_buffer,'(i10)') cmd_str_len
  fmt_np    = '(i'//adjustl(trim(char_buffer))//')'
  fmt_order = '(i'//adjustl(trim(char_buffer))//')'

  call MPI_INIT(ierr)

  !model_name="onetep_t27"
  !model_name = "onetep_t28"
  !model_name="data_onetep_ls/vacuum"
  !model_name="data_onetep_ls/solvent"

  call MPI_COMM_SIZE(mpi_comm_world, nproc, ierr)
  call MPI_COMM_RANK(mpi_comm_world, myid, ierr)

  if (command_argument_count() /= nargs) then
     write(ERROR_UNIT,'(a,i0,a,i0,a)') "Incorrect number of command line arguments.&
       & Expect ",nargs," but received ",command_argument_count(),". Aborting."
     call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
  end if

  ! First command-line argument is model name
  call get_command_argument(1,model_name)
  if ( myid == 0) then
     write(OUTPUT_UNIT,*)
     write(OUTPUT_UNIT,'(a,t25,a)') 'Model : ', trim(model_name)
  endif

  ! Command line arguments 2..4 are number of MPI ranks in each of x, y, z
  ! directions for Cartesian topology MPI communicator
  do i=1,3
    call get_command_argument(i+1,np_str(i))
    read(np_str(i),fmt_np) np(i)
  end do
  if ( myid == 0 ) then
    write(OUTPUT_UNIT,'(a,t25,i0,2x,i0,2x,i0,2x)') "MPI topology : ", np(1), np(2), np(3)
  end if

  if ( myid == 0) then
     write(OUTPUT_UNIT,'(a,t25,i0)') "Aggregation level : ", agg_level
  endif

  ! Command line argument 5 is the order of high-order finite differences used
  ! by the defect correction
  call get_command_argument(5,order_str)
  read(order_str,fmt_order) order
  if ( myid == 0) then
     write(OUTPUT_UNIT,'(a,t25,i0)') "FD order : ", order
     write(OUTPUT_UNIT,*)
  endif

  select case(model_name)
  case("onetep_t27") ! QC test 27
     pq1f = 161
     pq2f = 161
     pq3f = 161
     npt1 = 168
     npt2 = 168
     npt3 = 168
     d1f  = 0.285714285714286_wp
     d2f  = 0.285714285714286_wp
     d3f  = 0.285714285714286_wp
     nfiles = 2
  !case("onetep_t28")
  !   pq1f = 193
  !   pq2f = 193
  !   pq3f = 193
  !   d1f  = 0.221938775510204_wp
  !   d2f  = 0.221938775510204_wp
  !   d3f  = 0.221938775510204_wp
  !   nfiles = 3
  !case("data_onetep_ls/vacuum")
  !  pq1f = 577
  !  pq2f = 577
  !  pq3f = 673
  !  d1f  = 0.25_wp
  !  d2f  = 0.25_wp
  !  d3f  = 0.25_wp
  !  nfiles = 2
  !case("data_onetep_ls/solvent")
  !   pq1f = 641
  !   pq2f = 641
  !   pq3f = 737
  !   d1f  = 0.225155279503106_wp
  !   d2f  = 0.225155279503106_wp
  !   d3f  = 0.224734042553191_wp
  !   nfiles = 3
  !case("lysozyme")
  !   pq1f = 449
  !   pq2f = 545
  !   pq3f = 609
  !   d1f  = 0.225155279503106_wp
  !   d2f  = 0.225155279503106_wp
  !   d3f  = 0.224734042553191_wp
  !   nfiles = 2
  case default
    write(ERROR_UNIT,*) "Unknown model, aborting."
    call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
  end select

  ! Prepare variables in format suitable for dl_mg_defco_init
  n  = [ pq1f, pq2f, pq3f ]
  d  = [ d1f,  d2f,  d3f  ]
  bc = [ DL_MG_BC_DIRICHLET, DL_MG_BC_DIRICHLET, DL_MG_BC_DIRICHLET ]

  if (nproc > product(np) ) then
    ! More MPI ranks then parallel elements in user-specified topology.
    ! Some ranks will need to be inactive.
    nproc_truncated = product(np)
  else if (nproc == product(np) ) then
    ! All MPI ranks can have slab data
    nproc_truncated = nproc
  else
    ! Too few MPI ranks provided to satisfy user-requested topology
    write(ERROR_UNIT,*) "Number of MPI ranks is less than required to &
       &satisfy the user-specified topology. Aborting."
    call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
  end if


  ! Identify rank without data and exclude these from the Cartesian
  ! topology communicator (see comms_cart_create)
  if (myid >= nproc_truncated) then
    color = MPI_UNDEFINED
  else
    color = 1
  end if
  ! This has the effect that MPI ranks with color = MPI_UNDEFINED have
  ! local_comm = MPI_COMM_NULL, while a new communicator is created for the
  ! remaining MPI ranks.
  call MPI_COMM_SPLIT(MPI_COMM_WORLD, color, myid, local_comm, ierr)

  ! Create Cartesian topology
  if (local_comm /= MPI_COMM_NULL) then
     ! Active MPI ranks
     call MPI_COMM_SIZE(local_comm, nproc_active, ierr)
     call MPI_CART_CREATE(local_comm, 3, (/ np(1), np(2), np(3) /), &
          (/ .false., .false., .false. /), .false., mg_comm,ierr)
  else
     ! Inactive MPI ranks
     mg_comm = MPI_COMM_NULL
  end if

  if (mg_comm /= MPI_COMM_NULL) then
    ! All MPI ranks calculate the same global map of data (avoid communication)
    allocate(global_map(6,0:nproc_active-1), stat=ierr)
    ! [ [isx, iex, isy, iey, isz, iez ], [0 .. num_ranks-1 ] ]
    allocate(global_npoints(3,0:nproc_active-1), stat=ierr)
    ! [ [nx, ny, nz ], [0 .. num_ranks-1 ] ]

    ! Determine number of grid points along each Cartesian direction to distribute
    ! to each MPI rank.
    do irank = 0, nproc_active-1
      call MPI_CART_COORDS(mg_comm,irank,3,np_count,ierr)
      do icart = 1, 3
        ! Divide only inner points among MPI ranks, to avoid having more MPI ranks
        ! than inner grid points (excluding edges)
        global_npoints(icart,irank) = (n(icart) - 2) / np(icart)
        ! Distribute remainder points
        ! --> If the coordinate in a Cartesian direction is less than the remainder
        !     from dividing the inner points in that direction by the number of
        !     ranks along that direction, then add a point.
        ! --> This effectively adds extra 2-D slabs in each Cartesian direction
        !     for a subset of MPI ranks
        if ( np_count(icart) < mod(n(icart)-2,np(icart))) then
          global_npoints(icart,irank) = global_npoints(icart,irank) + 1
        end if
      end do
    end do

    ! Distribute edge points
    ! --> If the MPI rank is on the edge of Cartesian topology, then it must
    !     receive 1 point in the direction normal to that edge.
    ! Loop over Cartesian directions and determine which ranks receive edge points

    ! icart is Cartesian direction normal to surface
    do icart = 1, 3
      jcart = icart + 1
      kcart = icart + 2
      if (jcart > 3) jcart = modulo(jcart,3)
      if (kcart > 3) kcart = modulo(kcart,3)
      ! "Bottom" surfaces (where at least one of x, y or z coord = 0)
      ! [ 0, j, k ]
      ! [ j, 0, k ]
      ! [ j, k, 0 ]
      !if (myid == 0 ) write(OUTPUT_UNIT,*) icart, jcart, kcart
      np_count(icart) = 0
      ! jcart and kcart are the Cartesian coordinates of the surface
      do j = 0, np(jcart)-1
        do k = 0, np(kcart)-1
          np_count(jcart) = j
          np_count(kcart) = k
          !if (myid == 0 ) write(OUTPUT_UNIT,*) "bot", myid, np_count
          call MPI_CART_RANK(mg_comm,np_count,irank,ierr)
          global_npoints(icart,irank) = global_npoints(icart,irank) + 1
        end do
      end do
      ! "Top" surfaces (where at least one of x, y or z coord = np)
      ! [ np(x)-1, j, k ]
      ! [ j, np(y)-1, k ]
      ! [ j, k, np(z)-1 ]
      np_count(icart) = np(icart)-1
      ! jcart and kcart are the Cartesian coordinates of the surface
      do j = 0, np(jcart)-1
        do k = 0, np(kcart)-1
          np_count(jcart) = j
          np_count(kcart) = k
          !if( myid == 0 )write(OUTPUT_UNIT,*) "top", myid, np_count
          call MPI_CART_RANK(mg_comm,np_count,irank,ierr)
          global_npoints(icart,irank) = global_npoints(icart,irank) + 1
        end do
      end do
    end do

    ! Check total points distributed between ranks is equal to total number
    ! of points in full volumetric data
    total_points = 0
    do irank = 0, nproc_active-1
       total_points = total_points + product(global_npoints(:,irank))
    end do

    if (total_points /= product(n) ) then
       write(ERROR_UNIT,'(a,i0,a,i0,a,i0,a,i0,a,i0,a,i0,a)') &
            "Total grid points distributed is ",total_points,&
            " but should be ", product(n)
       call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
    end if

    ! Check that the points along each direction, for each line along the
    ! x, y, z directions of the Cartesian topology is equal to the points
    ! of the full volumetric data in that direction
    do icart = 1, 3
      jcart = icart + 1
      kcart = icart + 2
      if (jcart > 3) jcart = modulo(jcart,3)
      if (kcart > 3) kcart = modulo(kcart,3)
      do k = 0, np(kcart)-1
        do j = 0, np(jcart)-1
          np_count(jcart) = j
          np_count(kcart) = k
          line_sum = 0
          do i = 0, np(icart)-1
            np_count(icart) = i
            call MPI_CART_RANK(mg_comm,np_count,irank,ierr)
            line_sum = line_sum + global_npoints(icart,irank)
          end do
          if (line_sum /= n(icart)) then
             write(ERROR_UNIT,'(a,i0,a,i0,a,i0,a,i0,a,i0,a,i0,a)') &
                  "Sum of grid points along icart = ",icart,&
                  " is ", line_sum, " but should be ", n(icart), &
                  " for MPI rank with coordinates (",&
                  np_count(1),",",np_count(2),",",np_count(3),")."
             call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
          end if
        end do
      end do
    end do

    ! Generate global_map using global_npoints and coordinates of Cartesian topology
    k = 1
    do iz = 0, np(3)-1
      j = 1
      do iy = 0, np(2)-1
        i = 1
        do ix = 0, np(1)-1
          np_count = [ ix, iy, iz ]
          call MPI_CART_RANK(mg_comm,np_count,irank,ierr)
          ! [ [isx, iex, isy, iey, isz, iez ], [0 .. num_ranks-1 ] ]
          global_map(1,irank) = i; global_map(2, irank) = i + global_npoints(1,irank) - 1
          global_map(3,irank) = j; global_map(4, irank) = j + global_npoints(2,irank) - 1
          global_map(5,irank) = k; global_map(6, irank) = k + global_npoints(3,irank) - 1
          i = i + global_npoints(1,irank)
        end do
        j = j + global_npoints(2,irank)
      end do
      k = k + global_npoints(3,irank)
    end do
  end if
  if ( myid == 0 ) then
    write(OUTPUT_UNIT,'(a,i0)') "Total grid points in data:     ", product(n)
    write(OUTPUT_UNIT,'(a,i0)') "Total grid points distributed: ", total_points
    write(OUTPUT_UNIT,'(a,i0)') "MPI ranks active:              ", nproc_active
    do irank = 0, nproc_active-1
      write(OUTPUT_UNIT,'(a,i4,a,i0,4x,a,3i6,a)') &
                                "--> Grid points on rank ",irank,":     ",&
        product( global_npoints(:,irank) ), "[", global_npoints(:,irank), " ]"
    end do

    do irank = 0, nproc_active-1
      write(OUTPUT_UNIT,'(a,i4,a,6i6)') &
                                "--> Start/end coords on rank ",irank,":",&
        global_map(:,irank)
    end do
  end if

  ! At this point, every (active) MPI rank should have the same global_npoints
  ! and global_map arrays, containing the extents and coordinates of the data
  ! to be held by each MPI rank
  ! Now we set idx_s and idx_e for the current rank to pass to dl_mg_init
  ! Also set n_loc for the current rank to use in reading onetep data
  if (mg_comm /= MPI_COMM_NULL) then
     ! Active MPI ranks
     call MPI_COMM_RANK(mg_comm, myid, ierr)
     idx_s = [ global_map(1,myid), global_map(3,myid), global_map(5, myid) ]
     idx_e = [ global_map(2,myid), global_map(4,myid), global_map(6, myid) ]
     n_loc = global_npoints(1:3,myid)
  else
     ! Inactive MPI ranks
     idx_s = [ 0, 0, 0 ]
     idx_e = [ 0, 0, 0 ]
     n_loc = 0
  end if


  ! Check that the value of np(i) does not exceed pq{i}f-2 (i = 1, 2, 3)
  if ( np(1) > pq1f-2 .or. np(2) > pq2f-2 .or. np(3) > pq3f-2 ) then
     write(ERROR_UNIT,*) "Number of MPI ranks in one or more Cartesian &
       &directions, np(i), exceeds pq{i}f-2."
     call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
  end if


  ! Synchronize timetag for all MPI ranks, so that all use the same output_rootname
  if (mg_comm /= MPI_COMM_NULL) then
    if (myid == 0) call date_and_time(time=timetag)
    call MPI_BCAST( timetag, 10, MPI_CHARACTER, 0, mg_comm, ierr )
  end if

  !call dl_mg_init(pq1f, pq2f, pq3f, &
  !     d1f, d2f, d3f, &
  !     (/ DL_MG_BC_DIRICHLET, DL_MG_BC_DIRICHLET, DL_MG_BC_DIRICHLET /), &
  !     idx_s, idx_e, &
  !     150, 98, "dl_mg_report_"//timetag//".txt", mg_comm,&
  !     full_aggregation_level=agg_level)

  call dl_mg_defco_init(n, d, bc, idx_s, idx_e, 150, &
       "test_onetep_num_"//timetag, mg_comm, pbe_mode, &
       is_damping, error_tol, defect_tol, maxit_defco, &
       full_aggregation_level=agg_level, &
       output_mode=DL_MG_OUTPUT_MODE_VERBOSE, &
       debug_mode=DL_MG_DEBUG_MODE_ON)

  if (mg_comm /= MPI_COMM_NULL) then
    ! Allocate space for potential, density, boundary conditions and dielectric
    ! constant (on 'full' and 'half' grid points) on local rank
    allocate( pot(n_loc(1), n_loc(2), n_loc(3)), rho(n_loc(1), n_loc(2), n_loc(3)) )
    allocate( bound(n_loc(1), n_loc(2), n_loc(3)), eps_half(n_loc(1), n_loc(2), n_loc(3),3))
    allocate( eps_full(n_loc(1), n_loc(2), n_loc(3)) )
    !allocate( res(n_loc(1), n_loc(2), n_loc(3)))
    if ( nfiles == 2) then
       eps_half(:,:,:,:) = 1.d0
       eps_full(:,:,:) = 1.d0
    endif

    ! Read ONETEP data from file
    call read_onetep_data

    ! Deallocate global_map and global_npoints, now that data has been read and
    ! distributed to all MPI ranks.
    ! These were allocated only for active MPI ranks, so we can deallocate inside
    ! this if..end if block.
    deallocate(global_map, stat=ierr)
    deallocate(global_npoints, stat=ierr)

    ! Norm of source term
    call MPI_REDUCE(sum(rho(1:n_loc(1),1:n_loc(2),1:n_loc(3))**2),rho_norm,1,mpi_double_precision,MPI_SUM,0,mg_comm,ierr)

    ! Potential before application of solver
    call MPI_REDUCE(sum(bound(1:n_loc(1),1:n_loc(2),1:n_loc(3))**2),pot_norm_before,1,mpi_double_precision,MPI_SUM,0,mg_comm,ierr)

    t1 = mpi_wtime()
    !call dl_mg_solver_poisson(eps_half, -4.0d0*pi, rho, pot, tol=1.d-5, res=res)
    call dl_mg_defco_defect_corr_solver(pot, rho, order, bound, eps_full=eps_full, eps_half=eps_half)
    t2 = mpi_wtime()

    ! Potential after application of solver
    call MPI_REDUCE(sum(pot(1:n_loc(1),1:n_loc(2),1:n_loc(3))**2),pot_norm_after,1,mpi_double_precision,MPI_SUM,0,mg_comm,ierr)

    call MPI_REDUCE(t2-t1,tsolver_max, 1, mpi_double_precision, MPI_MAX, 0, mg_comm,ierr)
    call MPI_REDUCE(t2-t1,tsolver_min, 1, mpi_double_precision, MPI_MIN, 0, mg_comm, ierr)
    call MPI_REDUCE(t2-t1,tsolver_sum, 1, mpi_double_precision, MPI_SUM, 0, mg_comm, ierr)

    deallocate( pot, rho )
    deallocate( bound, eps_half)
    deallocate( eps_full)
  end if


  if (myid == 0)  then
     write(OUTPUT_UNIT,'(a,e23.15)') 'done, norm rho                    = ', sqrt(rho_norm)
     write(OUTPUT_UNIT,'(a,e23.15)') 'done, norm pot before solver call = ', sqrt(pot_norm_before)
     write(OUTPUT_UNIT,'(a,e23.15)') 'done, norm pot after solver call  = ', sqrt(pot_norm_after)
     write(OUTPUT_UNIT,*) 'solver times:    max         min        ave'
     write(OUTPUT_UNIT,'(I4,11x,3(E10.4,1x))') nproc_active, tsolver_max, tsolver_min, tsolver_sum/nproc_active
     write(OUTPUT_UNIT,'(i0,a,i0,a)') nproc_active, " of ", nproc, " MPI ranks used by solver"
  end if


  call MPI_FINALIZE(ierr)

  contains

    subroutine read_onetep_data
      implicit none
      integer i, j, fh, nt, ierr
      integer i1, i2, i3
      integer irank
      integer zstart, zend
      integer ftype, ftype_rho, ftype_eps
      integer status(MPI_STATUS_SIZE), nr

      integer, parameter :: in_unit = 42
      integer, parameter :: dummy_tag = 0

      integer(kind=MPI_OFFSET_KIND) disp
      character(len=128) fname
      real(wp) xmin, xmax
      logical :: is_opened
      integer :: ncart
      !
      integer :: isx, iex, isy, iey, isz, iez !< Local names for start and end
                                              !! indexes for local data on this rank
      integer :: isx_r, iex_r, isy_r, iey_r, isz_r, iez_r !< Local names for start and end
                                                          !! indexes for local data on OTHER ranks
      integer :: nx_r, ny_r, nz_r !< Local for extents of local data on OTHER ranks

      real(kind=wp), allocatable :: grid_data(:,:,:,:) !< Contains all data from
                                                       !! truncated grid
      real(kind=wp), allocatable :: send_buffer(:) !< Buffer for sending data
                                 !! to avoid temporary copies of array slices
                                 !! http://mpi-forum.org/docs/mpi-2.0/mpi-20-html/node236.htm
      integer :: x_max, y_max, z_max !< Maximum extents of local data held on any MPI rank

      ! Set local variables for start/end of data on local MPI rank
      isx = global_map(1,myid)
      iex = global_map(2,myid)
      isy = global_map(3,myid)
      iey = global_map(4,myid)
      isz = global_map(5,myid)
      iez = global_map(6,myid)

      !write(ERROR_UNIT,*) 'zsizes ', zsizes, myid


      ! Root MPI rank reads in data files, then distributes to other ranks
      if (myid == 0) then
        ! Allocate a send buffer with the extents of the largest data chunk
        ! on any MPI rank
        x_max = maxval( global_npoints(1,:) )
        y_max = maxval( global_npoints(2,:) )
        z_max = maxval( global_npoints(3,:) )

        ! Allocate contiguous send buffer
        allocate( send_buffer(x_max*y_max*z_max), stat=ierr )

        do i = 1, nfiles ! 3 if eps_half is not constant

           inquire(unit=in_unit,opened=is_opened)
           if (is_opened) then
             write(ERROR_UNIT,*) "in_unit is already opened, aborting."
             call MPI_ABORT(mpi_comm_world, MPI_ERR_UNKNOWN, ierr)
           end if


           select case(i)
           case (1)
             ! Starting guess for potential (boundary conditions)
             ncart = 1
             fname=trim(model_name)//'/bound.dx'
           case(2)
             ! Source term (charge density)
             ncart = 1
             fname=trim(model_name)//'/rho.dx'
           case(3)
             ncart = 3
             ! Permittivity values (at half distance between grid points for
             ! density and potential)
             fname=trim(model_name)//'/eps_half.dx'
           end select

           allocate(grid_data(pq1f,pq2f,pq3f,ncart))

           call read_dx_data(in_unit,fname,npt1,npt2,npt3,&
                             pq1f,pq2f,pq3f,&
                             grid_data(:,:,:,1))

           ! Distribute data between MPI ranks
           ! Get data for myself (rank zero)
           select case(i)
           case (1)
             ! Starting guess for potential (boundary conditions)
             ! Give myself the data for rank zero
             bound = grid_data(isx:iex,isy:iey,isz:iez,1)
           case(2)
             ! Source term (charge density)
             ! Give myself the data for rank zero
             rho = grid_data(isx:iex,isy:iey,isz:iez,1)
           case(3)
             ! Permittivity values (at half distance between grid points for
             ! density and potential)
           end select

           ! Send to other MPI ranks /= 0 (with tag=i, number of file)
           do irank = 1, nproc_active-1
             isx_r = global_map(1,irank)
             iex_r = global_map(2,irank)
             isy_r = global_map(3,irank)
             iey_r = global_map(4,irank)
             isz_r = global_map(5,irank)
             iez_r = global_map(6,irank)
             nx_r   = global_npoints(1,irank)
             ny_r   = global_npoints(2,irank)
             nz_r   = global_npoints(3,irank)
             nt     = nx_r*ny_r*nz_r
             ! Fill send buffer
             do iz = 0, nz_r - 1
               do iy = 0, ny_r - 1
                 do ix = 0, nx_r - 1
                   send_buffer(1 + ix + iy * nx_r + iz * nx_r * ny_r) = &
                     grid_data(isx_r + ix, isy_r + iy, isz_r + iz, 1)
                 end do
               end do
             end do
             ! Use a blocking sent to avoid issues with temporary copies of
             ! array slices being destroyed before they are received
             ! ... all the other MPI ranks will have reached the MPI_RECV
             !     call below while the root rank is reading the data.
             write(*,*) "send", i, irank, nt, sum(send_buffer(1:nt))**2
             call MPI_SEND(send_buffer,nt,&
                  MPI_DOUBLE_PRECISION,irank,i,mg_comm,ierr)
           end do

           deallocate(grid_data, stat=ierr)
        end do
        ! Deallocate contiguous send buffer
        !deallocate(send_buffer, stat=ierr)

      else
        ! Receive data from rank zero (eventually!)
        nt = product( n_loc )
        do i = 1, nfiles
           ! i is the MPI tag, differentiating between files.
           ! Rank 0 will wait for all receives to complete for each file before
           ! proceeding to the next file
           select case(i)
           case(1)
             ! Starting guess for potential (boundary conditions)
             call MPI_RECV(bound,nt,MPI_DOUBLE_PRECISION,0,1,mg_comm,MPI_STATUS_IGNORE,ierr)
             write(*,*) "recv", i, myid, nt, sum(bound)**2
           case(2)
             ! Source term (charge density)
             call MPI_RECV(rho,nt,MPI_DOUBLE_PRECISION,0,2,mg_comm,MPI_STATUS_IGNORE,ierr)
             write(*,*) "recv", i, myid, nt, sum(rho)**2
           case(3)
             ! Permittivity values (at half distance between grid points for
             ! density and potential)
             !write(*,*) "recv", nt
             !call MPI_RECV(pot,nt,MPI_DOUBLE_PRECISION,0,3,mg_comm,MPI_STATUS_IGNORE,ierr)
           end select
        end do
      end if

    end subroutine read_onetep_data

    !> Read a DX format data file.
    !! Assumes the exact format output by ONETEP's visual_output_dx routine
    !! (dx_format_coarse F, dx_format_digits 16 )
    subroutine read_dx_data(input_file_unit,input_filename,n1,n2,n3,&
                            pq1f,pq2f,pq3f,&
                            grid_data)
      implicit none

      integer, intent(in) :: input_file_unit
      character(len=*), intent(in) :: input_filename
      integer, intent(in) :: n1, n2, n3  !< Full grid extents (from ONETEP)
      integer, intent(in) :: pq1f, pq2f, pq3f !< Truncated grid extents (for multigrid)
      real(kind=wp),intent(out) :: grid_data(pq1f,pq2f,pq3f)  !< All reals read from DX file in 3-D array
                                         !! with fourth dimension representing

      integer, parameter :: n_comment = 2
      integer, parameter :: n_header  = 7
      integer, parameter :: dx_sig_digits = 16
      integer, parameter :: max_entries_per_line = 3

      character(len=80) :: format_str, format_str1, format_str2, buffer
      logical :: buffer_mask(80)
      integer :: i, j, idata
      integer :: icart, i1, i2, i3
      integer :: n_lines_data
      integer :: n_entries
      character(len=80) :: n_entries_str
      real(kind=wp), allocatable :: dx_data(:)

      ! Build format statement for data based on number of significant digits
      ! as done in visual_output_dx
      write(format_str1,*) dx_sig_digits+7
      write(format_str2,*) dx_sig_digits-1
      format_str = &
           'ES'//trim(adjustl(format_str1))//'.'//trim(adjustl(format_str2))

      ! Open file for input
      open(unit=input_file_unit, file=input_filename, status="OLD", action="READ")

      ! * Skip initial comment lines
      ! * Skip the header lines
      do i = 1, n_comment + n_header
        read(input_file_unit,*)
      end do

      ! Allocate temporayr 1D array for DX data
      allocate(dx_data(npt1*npt2*npt3))

      ! Number of lines of data contained in file
      n_lines_data = ceiling(real(n1*n2*n3,kind=wp)/real(max_entries_per_line,kind=wp))
      idata = 1
      do i = 1, n_lines_data
        ! Within the truncated grid, store data
        ! Read line to character character buffers
        read(input_file_unit,'(a80)') buffer
        !write(OUTPUT_UNIT,*) i, buffer
        ! Determine number of reals in string (occurences of capital E)
        do j = 1, 80
          buffer_mask(j) = buffer(j:j).eq.'E'
        end do
        n_entries = count(buffer_mask)
        write(n_entries_str,'(i80)') n_entries
        !write(OUTPUT_UNIT,*) n_entries_str
        read(buffer,'('//trim(adjustl(n_entries_str))//format_str//')') &
          dx_data(idata:idata+n_entries-1)
        idata = idata + n_entries
      end do

      ! Copy data read from file into 3D array representing truncated grid
      do i1 = 1, pq1f
        do i2 = 1, pq2f
          do i3 = 1, pq3f
            grid_data(i1,i2,i3) = dx_data( i3 + (i2-1)*npt1 + (i1-1)*npt1*npt2 )
          end do
        end do
      end do

      ! Deallocate temporary array for DX data
      deallocate(dx_data)

      ! File reading complete, close file
      close(unit=input_file_unit)

    end subroutine read_dx_data

end program test_onetep_num
