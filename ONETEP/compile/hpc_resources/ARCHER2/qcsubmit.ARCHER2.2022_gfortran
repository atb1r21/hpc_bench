#!/bin/bash

# -------------------------------------------------
# A SLURM submission script for ONETEP QC-tests on ARCHER2.
# (GNU Fortran compiler version).
# Supports hybrid (MPI/OMP) parallelism.
# 2021.11 Jacek Dziedzic, J.Dziedzic@soton.ac.uk
#                         University of Southampton
# 2021.03 Lennart Gundelach, L.Gundelach@soton.ac.uk
#                            University of Southampton
# -------------------------------------------------

# v1.00 (2021.03.17) Submission script adapted from IRIDIS5 to ARCHER2.
# v1.01 (2021.05.13) Trivial fixes by jd.
# v1.10 (2021.11.15) Adapted by jd to the new "official" config file.
# v1.20 (2021.11.28) Adapted by jd to the new "official" config file for GNU Fortran.
# v1.21 (2021.11.29) jd: Minor fixes.
# v1.35 (2022.02.26) Uses 'close' binding to get good performance of threaded BLAS.

# ==========================================================================================================
# Do not edit this file, except for the "account" line, unless you know what you're doing.
# Put it in the 'tests' subdirectory of your ONETEP installation, change to that directory and
# submit it by issuing:
# sbatch qcsubmit.ARCHER2.2022_gfortran
# ==========================================================================================================
#
#SBATCH --job-name=QC_tests           # Name of the job.
#SBATCH --ntasks=4                    # Total number of MPI processes in job (might be adjusted down by testcode).
#SBATCH --nodes=1                     # Number of nodes in job.
#SBATCH --ntasks-per-node=4           # Number of MPI processes per node (might be adjusted down by testcode).
#SBATCH --cpus-per-task=8             # Number of OMP threads spawned from each MPI process.
#SBATCH --time=03:00:00               # Max time for your job (hh:mm:ss).

#SBATCH --partition=standard          # Queue. standard: CPU nodes with AMD EPYC 7742 64-core processor
#SBATCH --account=e89-soto            # Replace e89-soto with your budget code, if need be.
#SBATCH --qos=standard                # Requested Quality of Service (QoS), See ARCHER2 documentation

export OMP_NUM_THREADS=8              # Repeat the value from 'cpus-per-task' here.

# ---------------------------------------------------------------------------------------------------------
# ---------------------------------------------------------------------------------------------------------

module load PrgEnv-gnu
module load gcc/9.3.0
module load cray-fftw
module load cray-mpich/8.1.4

# These two are crucial to get decent performance of threaded libsci on Archer with the Cray compiler.
export OMP_PLACES=cores
export OMP_PROC_BIND=close

# Works around a memory leak in libfabric on Archer2.
export FI_MR_CACHE_MAX_COUNT=0

# This is needed to get the correct FFTW3 (Archer2 known issue).
export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH

workdir=`pwd`

# Ensure we are started from ONETEP's 'tests' directory.
if [ `echo $workdir | grep -E "/tests$"` ]; then
  onetepdir=`echo $workdir | sed -r "s%/tests$%%"`
else
  echo "!!! This script must be run from the ONETEP installation's tests directory. Aborting!" >&2
  touch "%WAS_RUN_FROM_WRONG_DIR"
  exit 2
fi

onetep_launcher="$onetepdir""/utils/onetep_launcher"

echo "--- This is the submission script, the time is `date`."
echo "--- workdir is '$workdir'."
echo "--- ONETEP's top-level directory is '$onetepdir'."
echo "--- onetep_launcher is '$onetep_launcher'."

# Ensure onetep_launcher is there and is indeed executable.
if [ ! -x "$onetep_launcher" ]; then
  echo "!!! $onetep_launcher does not exist or is not executable. Aborting!" >&2
  touch "%ONETEP_LAUNCHER_MISSING"
  exit 5
fi

# Dump the module list to a file.
module list >\$modules_loaded 2>&1

# Report details
echo "--- Number of nodes as reported by SLURM: $SLURM_JOB_NUM_NODES."
echo "--- Number of tasks as reported by SLURM: $SLURM_NTASKS."
echo "--- Using this srun executable: "`which srun`
echo "--- Executing ONETEP via $onetep_launcher." 

# Tidy any previous runs.
echo "y" | ./testcode/bin/testcode.py --older-than=0 tidy >/dev/null

# Ensure we run via 'srun', on 1 node and a suitable number of MPI ranks (from 'userconfig' and 'jobconfig').
# Additional srun options to pin one thread per physical core
parallel_prefix="srun --hint=nomultithread --distribution=block:block -N 1 -n tc.nprocs"

# Actually run testcode.
########################################################################################################################################################
./testcode/bin/testcode.py -v -e ../utils/onetep_launcher --user-option onetep launch_parallel "$parallel_prefix" --user-option onetep run_cmd_template 'tc.program tc.args -t $OMP_NUM_THREADS tc.input >tc.output 2>tc.error'
########################################################################################################################################################

echo "--- srun finished at `date`."

# Check for error conditions
result=$?
if [ $result -ne 0 ]; then
  echo "!!! srun reported a non-zero exit code $result. Aborting!" >&2
  touch "%SRUN_ERROR"
  exit 6
fi

echo "--- Looks like everything went fine. Praise be."
touch "%DONE"

echo "--- Finished running QC tests at `date`. Examine the .out file to see if they passed."
