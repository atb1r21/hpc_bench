#!/bin/bash

# -----------------------------------------------------------------------------------
# A SLURM submission script for ONETEP QC-tests on ARCHER2 (full 23-cabinet system).
# Central install, Cray compiler version.
# Supports hybrid (MPI/OMP) parallelism.
#
# 2022.06 Jacek Dziedzic, J.Dziedzic@soton.ac.uk
#                         University of Southampton
# 2021.03 Lennart Gundelach, L.Gundelach@soton.ac.uk
#                            University of Southampton
# -------------------------------------------------

# v1.00 (2022.06.04) jd: Adapted from the user-compiled Cray compiler version.

# ==========================================================================================================
# Do not edit this file, except for the "account" line, unless you know what you're doing.
# Put it in the 'tests' subdirectory of your ONETEP installation, change to that directory and
# submit it by issuing:
# sbatch qcsubmit.ARCHER2.2022_central
# ==========================================================================================================
#
#SBATCH --job-name=QC_tests           # Name of the job.
#SBATCH --ntasks=4                    # Total number of MPI processes in job (might be adjusted down by testcode).
#SBATCH --nodes=1                     # Number of nodes in job.
#SBATCH --ntasks-per-node=4           # Number of MPI processes per node (might be adjusted down by testcode).
#SBATCH --cpus-per-task=16            # Number of OMP threads spawned from each MPI process.
#SBATCH --time=02:00:00               # Max time for your job (hh:mm:ss).
#SBATCH --partition=standard          # Queue. standard: CPU nodes with AMD EPYC 7742 64-core processor
#SBATCH --account=e89-soto            # Replace e89-soto with your budget code, if need be.
#SBATCH --qos=standard                # Requested Quality of Service (QoS), See ARCHER2 documentation

export OMP_NUM_THREADS=16             # Repeat the value from 'cpus-per-task' here.

# ---------------------------------------------------------------------------------------------------------
# ---------------------------------------------------------------------------------------------------------

# Set up the job environment, loading the ONETEP module.
# The module automatically sets OMP_PLACES, OMP_PROC_BIND and FI_MR_CACHE_MAX_COUNT.
# To use a different binary, replace this line with either (drop the leading '#')
# module load onetep/6.1.9.0-GCC-LibSci
# to use the GCC-libsci binary, or with
# module load onetep/6.1.9.0-GCC-MKL
# to use the GCC-MKL binary.

module load onetep/6.1.9.0-CCE-LibSci

workdir=`pwd`

# Ensure we are started from ONETEP's 'tests' directory.
if [ `echo $workdir | grep -E "/tests$"` ]; then
  onetepdir=`echo $workdir | sed -r "s%/tests$%%"`
else
  echo "!!! This script must be run from the ONETEP installation's tests directory. Aborting!" >&2
  touch "%WAS_RUN_FROM_WRONG_DIR"
  exit 2
fi

onetep_launcher="$onetepdir""/utils/onetep_launcher"

echo "--- This is the submission script, the time is `date`."
echo "--- workdir is '$workdir'."
echo "--- ONETEP's top-level directory is '$onetepdir'."
echo "--- onetep_launcher is '$onetep_launcher'."

# Ensure onetep_launcher is there and is indeed executable.
if [ ! -x "$onetep_launcher" ]; then
  echo "!!! $onetep_launcher does not exist or is not executable. Aborting!" >&2
  touch "%ONETEP_LAUNCHER_MISSING"
  exit 5
fi

# Dump the module list to a file.
module list >\$modules_loaded 2>&1

# Report details
echo "--- Number of nodes as reported by SLURM: $SLURM_JOB_NUM_NODES."
echo "--- Number of tasks as reported by SLURM: $SLURM_NTASKS."
echo "--- Using this srun executable: "`which srun`
echo "--- Executing ONETEP via $onetep_launcher." 

# Figure out ONETEP executable
onetep_exe=`which onetep.archer2`

echo "--- ONETEP executable is $onetep_exe."

if [ -z "$onetep_exe" ]; then
  echo "!!! Could not find ONETEP executable: " `which onetep.archer`
  touch "%ONETEP_EXE_MISSING"
  exit 7
fi

# Tidy any previous runs.
echo "y" | ./testcode/bin/testcode.py --older-than=0 tidy >/dev/null

# Ensure we run via 'srun', on 1 node and a suitable number of MPI ranks (from 'userconfig' and 'jobconfig').
# Additional srun options to pin one thread per physical core
parallel_prefix="srun --hint=nomultithread --distribution=block:block -N 1 -n tc.nprocs"

# Actually run testcode.
########################################################################################################################################################
export onetep_exe
./testcode/bin/testcode.py -v -e ../utils/onetep_launcher --user-option onetep launch_parallel "$parallel_prefix" --user-option onetep run_cmd_template 'tc.program tc.args -t $OMP_NUM_THREADS -e $onetep_exe tc.input >tc.output 2>tc.error'
########################################################################################################################################################

echo "--- srun finished at `date`."

# Check for error conditions
result=$?
if [ $result -ne 0 ]; then
  echo "!!! srun reported a non-zero exit code $result. Aborting!" >&2
  touch "%SRUN_ERROR"
  exit 6
fi

echo "--- Looks like everything went fine. Praise be."
touch "%DONE"

echo "--- Finished running QC tests at `date`. Examine the .out file to see if they passed."
