Running SLURM prolog script on red371.cluster.local
===============================================================================
Job started on Wed Jan 11 15:03:16 GMT 2023
Job ID          : 2388133
Job name        : onetep
WorkDir         : /mainfs/home/hpc/benchmarks-2022/ONETEP/data_input/testcase1
Command         : /mainfs/home/hpc/benchmarks-2022/ONETEP/data_input/testcase1/jobsubmit.iridis5.2021
Partition       : batch
Num hosts       : 2
Num cores       : 80
Num of tasks    : 20
Hosts allocated : red[371-372]
Job Output Follows ...
===============================================================================
Loading compiler version 2021.2.0
Loading mkl version 2021.2.0
Loading mpi version 2021.2.0
--- This is the submission script, the time is Wed Jan 11 15:03:17 GMT 2023.
--- ONETEP executable is '/home/hpc/benchmarks-2022/ONETEP/source/bin/onetep.iridis5.intel21.omp.scalapack'.
--- workdir is '/home/hpc/benchmarks-2022/ONETEP/data_input/testcase1'.
--- onetep_launcher is '/home/hpc/benchmarks-2022/ONETEP/source/utils/onetep_launcher'.
--- The input file is 4088.dat, the output goes to 4088.out and errors go to 4088.err.
--- Number of nodes as reported by SLURM: 2.
--- Number of tasks as reported by SLURM: 20.
--- Using this srun executable: /local/software/slurm/default/bin/srun
--- Executing ONETEP via /home/hpc/benchmarks-2022/ONETEP/source/utils/onetep_launcher.
slurmstepd-red371: error: *** JOB 2388133 ON red371 CANCELLED AT 2023-01-11T17:03:17 DUE TO TIME LIMIT ***
==============================================================================
Running epilogue script on red371.

Submit time  : 2023-01-11T15:03:15
Start time   : 2023-01-11T15:03:16
End time     : 2023-01-11T17:03:17
Elapsed time : 02:00:01 (Timelimit=02:00:00)

Job ID: 2388133
Cluster: i5
User/Group: hpc/jf
State: TIMEOUT (exit code 0)
Nodes: 2
Cores per node: 40
CPU Utilized: 00:00:09
CPU Efficiency: 0.00% of 6-16:01:20 core-walltime
Job Wall-clock time: 02:00:01
Memory Utilized: 113.75 GB (estimated maximum)
Memory Efficiency: 35.95% of 316.41 GB (158.20 GB/node)

