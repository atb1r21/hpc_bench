Running SLURM prolog script on red321.cluster.local
===============================================================================
Job started on Mon Dec 19 14:22:19 GMT 2022
Job ID          : 2325607
Job name        : coll-imb
WorkDir         : /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/8n
Command         : /mainfs/home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/8n/coll.slurm
Partition       : batch
Num hosts       : 8
Num cores       : 320
Num of tasks    : 320
Hosts allocated : red[321-328]
Job Output Follows ...
===============================================================================
#----------------------------------------------------------------
#    Intel(R) MPI Benchmarks 2021.3, MPI-1 part
#----------------------------------------------------------------
# Date                  : Mon Dec 19 14:22:21 2022
# Machine               : x86_64
# System                : Linux
# Release               : 3.10.0-1160.36.2.el7.x86_64
# Version               : #1 SMP Thu Jul 8 02:53:40 UTC 2021
# MPI Version           : 3.1
# MPI Thread Environment: 


# Calling sequence was: 

# /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/IMB-MPI1 -npmin 320 -msglen length.txt -mem 4 Allreduce Allgather Scatter Alltoall 

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT 
# MPI_Op                         :   MPI_SUM  
# 
# 

# List of Benchmarks to run:

# Allreduce
# Allgather
# Scatter
# Alltoall

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 320 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20      2574.28      2992.32      2727.88

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 320 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20    336825.77    365461.43    347853.57

#----------------------------------------------------------------
# Benchmarking Scatter 
# #processes = 320 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20       933.51     47916.45     42084.33

#----------------------------------------------------------------
# Benchmarking Alltoall 
# #processes = 320 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152            5   1973927.48   1976767.30   1975434.90


# All processes entering MPI_Finalize

==============================================================================
Running epilogue script on red321.

Submit time  : 2022-12-19T14:22:18
Start time   : 2022-12-19T14:22:19
End time     : 2022-12-19T14:22:54
Elapsed time : 00:00:35 (Timelimit=2-12:00:00)

Job ID: 2325607
Cluster: i5
User/Group: hpc/jf
State: COMPLETED (exit code 0)
Nodes: 8
Cores per node: 40
CPU Utilized: 02:54:06
CPU Efficiency: 93.27% of 03:06:40 core-walltime
Job Wall-clock time: 00:00:35
Memory Utilized: 439.85 GB (estimated maximum)
Memory Efficiency: 35.19% of 1.22 TB (3.91 GB/core)

