Running SLURM prolog script on red321.cluster.local
===============================================================================
Job started on Mon Dec 19 14:15:21 GMT 2022
Job ID          : 2325597
Job name        : coll-imb
WorkDir         : /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/2n
Command         : /mainfs/home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/2n/coll.slurm
Partition       : batch
Num hosts       : 2
Num cores       : 80
Num of tasks    : 80
Hosts allocated : red[321-322]
Job Output Follows ...
===============================================================================
#----------------------------------------------------------------
#    Intel(R) MPI Benchmarks 2021.3, MPI-1 part
#----------------------------------------------------------------
# Date                  : Mon Dec 19 14:15:26 2022
# Machine               : x86_64
# System                : Linux
# Release               : 3.10.0-1160.36.2.el7.x86_64
# Version               : #1 SMP Thu Jul 8 02:53:40 UTC 2021
# MPI Version           : 3.1
# MPI Thread Environment: 


# Calling sequence was: 

# /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/IMB-MPI1 -npmin 80 -msglen length.txt -mem 3 Allreduce Allgather Scatter Alltoall 

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT 
# MPI_Op                         :   MPI_SUM  
# 
# 

# List of Benchmarks to run:

# Allreduce
# Allgather
# Scatter
# Alltoall

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 80 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20      2182.26      2507.18      2344.30

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 80 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20     80597.70     85494.40     83593.84

#----------------------------------------------------------------
# Benchmarking Scatter 
# #processes = 80 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20      2482.89      8355.62      7613.29

#----------------------------------------------------------------
# Benchmarking Alltoall 
# #processes = 80 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20    280603.79    282150.32    281636.67


# All processes entering MPI_Finalize

==============================================================================
Running epilogue script on red321.

Submit time  : 2022-12-19T14:15:20
Start time   : 2022-12-19T14:15:21
End time     : 2022-12-19T14:15:39
Elapsed time : 00:00:18 (Timelimit=2-12:00:00)

Job ID: 2325597
Cluster: i5
User/Group: hpc/jf
State: COMPLETED (exit code 0)
Nodes: 2
Cores per node: 40
CPU Utilized: 00:17:28
CPU Efficiency: 72.78% of 00:24:00 core-walltime
Job Wall-clock time: 00:00:18
Memory Utilized: 2.52 MB
Memory Efficiency: 0.00% of 312.50 GB

