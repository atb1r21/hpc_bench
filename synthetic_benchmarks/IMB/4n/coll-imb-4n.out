Running SLURM prolog script on red321.cluster.local
===============================================================================
Job started on Mon Dec 19 14:19:01 GMT 2022
Job ID          : 2325602
Job name        : coll-imb
WorkDir         : /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/4n
Command         : /mainfs/home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/4n/coll.slurm
Partition       : batch
Num hosts       : 4
Num cores       : 160
Num of tasks    : 160
Hosts allocated : red[321-324]
Job Output Follows ...
===============================================================================
#----------------------------------------------------------------
#    Intel(R) MPI Benchmarks 2021.3, MPI-1 part
#----------------------------------------------------------------
# Date                  : Mon Dec 19 14:19:05 2022
# Machine               : x86_64
# System                : Linux
# Release               : 3.10.0-1160.36.2.el7.x86_64
# Version               : #1 SMP Thu Jul 8 02:53:40 UTC 2021
# MPI Version           : 3.1
# MPI Thread Environment: 


# Calling sequence was: 

# /home/hpc/benchmarks-2022/IMB/mpi-benchmarks-IMB-v2021.3/IMB-MPI1 -npmin 160 -msglen length.txt -mem 3 Allreduce Allgather Scatter Alltoall 

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT 
# MPI_Op                         :   MPI_SUM  
# 
# 

# List of Benchmarks to run:

# Allreduce
# Allgather
# Scatter
# Alltoall

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 160 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20      2594.32      2844.46      2724.74

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 160 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20    166444.40    171265.82    169236.30

#----------------------------------------------------------------
# Benchmarking Scatter 
# #processes = 160 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           20      6747.63     20742.36     17436.45

#----------------------------------------------------------------
# Benchmarking Alltoall 
# #processes = 160 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
      2097152           12    838665.14    840269.62    839435.20


# All processes entering MPI_Finalize

==============================================================================
Running epilogue script on red321.

Submit time  : 2022-12-19T14:19:00
Start time   : 2022-12-19T14:19:01
End time     : 2022-12-19T14:19:27
Elapsed time : 00:00:26 (Timelimit=2-12:00:00)

Job ID: 2325602
Cluster: i5
User/Group: hpc/jf
State: COMPLETED (exit code 0)
Nodes: 4
Cores per node: 40
CPU Utilized: 00:59:08
CPU Efficiency: 85.29% of 01:09:20 core-walltime
Job Wall-clock time: 00:00:26
Memory Utilized: 2.52 MB
Memory Efficiency: 0.00% of 625.00 GB

